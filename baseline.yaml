HParams:
  run_name: "1_DISC _ New 3_3_3 modellam=0 n=10"
  log_directory: "/mnt/home/taylcard/dev/logs/debug/"
  num_epochs: 20
  gen_weight: 0
  kl_weight: 0.15
  disc_warmup: 10
  batch_size: 256
  mmvae_lr: 0.001
  disc_lr: 0.001
  seed: 42

Shared_VAE:
  encoder:
    - layer: Linear
      in: 448
      out: 384
    - layer: BatchNorm1d
      in: 384
    - activation: ReLU
    - layer: Linear
      in: 384
      out: 320
    - layer: BatchNorm1d
      in: 320
    - activation: ReLU
    - layer: Linear
      in: 320
      out: 256
      activation: ReLU
  decoder:
    - layer: Linear
      in: 256
      out: 320
    - layer: BatchNorm1d
      in: 320
    - activation: ReLU
    - layer: Linear
      in: 320
      out: 384
    - layer: BatchNorm1d
      in: 384
      activation: ReLU
    - layer: Linear
      in: 384
      out: 448
    - activation: ReLU
  mu:
    - layer: Linear
      in: 256
      out: 256
  logvar:
    - layer: Linear
      in: 256
      out: 256

Human_Expert:
  encoder:
    - layer: Linear
      in: 60_664
      out: 512
    - layer: BatchNorm1d
      in: 512
    - activation: ReLU
    - layer: Linear
      in: 512
      out: 480
    - layer: BatchNorm1d
      in: 480
    - activation: ReLU
    - layer: Linear
      in: 480
      out: 448
    - activation: ReLU
  decoder:
    - layer: Linear
      in: 448
      out: 480
    - layer: BatchNorm1d
      in: 480
    - activation: ReLU
    - layer: Linear
      in: 480
      out: 512
    - layer: BatchNorm1d
      in: 512
    - activation: ReLU
    - layer: Linear
      in: 512
      out: 60_664
      activation: ReLU

Mouse_Expert:
  encoder:
    - layer: Linear
      in: 52_417
      out: 512
    - layer: BatchNorm1d
      in: 512
    - activation: ReLU
    - layer: Linear
      in: 512
      out: 480
    - layer: BatchNorm1d
      in: 480
    - activation: ReLU
    - layer: Linear
      in: 480
      out: 448
    - activation: ReLU
  decoder:
    - layer: Linear
      in: 448
      out: 480
    - layer: BatchNorm1d
      in: 480
    - activation: ReLU
    - layer: Linear
      in: 480
      out: 512
    - layer: BatchNorm1d
      in: 512
    - activation: ReLU
    - layer: Linear
      in: 512
      out: 52_417
      activation: ReLU

Discriminator0:
  model:
    - layer: Linear
      in: 384
      out: 128
    - layer: BatchNorm1d
      in: 128
    - activation: Sigmoid
    - layer: Linear
      in: 128
      out: 64
      activation: Sigmoid
    - layer: Linear
      in: 64
      out: 1 
      activation: Sigmoid

Discriminator1:
  model:
    - layer: Linear
      in: 320
      out: 128
    - layer: BatchNorm1d
      in: 128
    - activation: Sigmoid
    - layer: Linear
      in: 128
      out: 64
      activation: Sigmoid
    - layer: Linear
      in: 64
      out: 1 
      activation: Sigmoid

Discriminator2:
  model:
    - layer: Linear
      in: 256
      out: 128
    - layer: BatchNorm1d
      in: 128
    - activation: Sigmoid
    - layer: Linear
      in: 128
      out: 64
      activation: Sigmoid
    - layer: Linear
      in: 64
      out: 1 
      activation: Sigmoid

